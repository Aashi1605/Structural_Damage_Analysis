{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-01T05:11:10.368145Z","iopub.execute_input":"2021-10-01T05:11:10.368824Z","iopub.status.idle":"2021-10-01T05:11:10.532522Z","shell.execute_reply.started":"2021-10-01T05:11:10.368722Z","shell.execute_reply":"2021-10-01T05:11:10.531718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nprint(\"Libraries imported - ready to use PyTorch\", torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:10.534451Z","iopub.execute_input":"2021-10-01T05:11:10.534735Z","iopub.status.idle":"2021-10-01T05:11:15.061284Z","shell.execute_reply.started":"2021-10-01T05:11:10.534697Z","shell.execute_reply":"2021-10-01T05:11:15.060558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\n# function to resize image\ndef resize_image(src_image, size=(128,128), bg_color=\"white\"): \n    from PIL import Image, ImageOps \n    \n    # resize the image so the longest dimension matches our target size\n    src_image.thumbnail(size, Image.ANTIALIAS)\n    \n    # Create a new square background image\n    new_image = Image.new(\"RGB\", size, bg_color)\n    \n    # Paste the resized image into the center of the square background\n    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n  \n    # return the resized image\n    return new_image","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:15.06265Z","iopub.execute_input":"2021-10-01T05:11:15.063242Z","iopub.status.idle":"2021-10-01T05:11:15.070435Z","shell.execute_reply.started":"2021-10-01T05:11:15.0632Z","shell.execute_reply":"2021-10-01T05:11:15.069712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\ntraining_folder_name = '../input/structural-damage/data_sample/train'\n\n# New location for the resized images\ntrain_folder = '../working/images'\n\nos.listdir(training_folder_name)\n# Create resized copies of all of the source images\nsize = (128,128)\n\n# Create the output folder if it doesn't already exist\nif os.path.exists(train_folder):\n    shutil.rmtree(train_folder)\n\n# Loop through each subfolder in the input folder\nprint('Transforming images...')\nfor root, folders,files in os.walk(training_folder_name):\n    print(\"HERE\")\n    for sub_folder in folders:\n        \n        print('processing folder ' + sub_folder)\n        # Create a matching subfolder in the output dir\n        saveFolder = os.path.join(train_folder,sub_folder)\n        if not os.path.exists(saveFolder):\n            os.makedirs(saveFolder)\n        # Loop through the files in the subfolder\n        file_names = os.listdir(os.path.join(root,sub_folder))\n        for file_name in file_names:\n            # Open the file\n            file_path = os.path.join(root,sub_folder, file_name)\n            #print(\"reading \" + file_path)\n            image = Image.open(file_path)\n            # Create a resized version and save it\n            resized_image = resize_image(image, size)\n            saveAs = os.path.join(saveFolder, file_name)\n            #print(\"writing \" + saveAs)\n            resized_image.save(saveAs)\n\nprint('Done.')","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:15.071938Z","iopub.execute_input":"2021-10-01T05:11:15.072284Z","iopub.status.idle":"2021-10-01T05:11:15.764932Z","shell.execute_reply.started":"2021-10-01T05:11:15.072248Z","shell.execute_reply":"2021-10-01T05:11:15.763371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset(data_path):\n    import torch\n    import torchvision\n    import torchvision.transforms as transforms\n    # Load all the images\n    transformation = transforms.Compose([\n        # Randomly augment the image data\n            # Random horizontal flip\n        transforms.RandomHorizontalFlip(0.5),\n            # Random vertical flip\n        transforms.RandomVerticalFlip(0.3),\n        # Randomly change brightness, saturation etc of an image\n        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0),\n        # transform to tensors\n        transforms.ToTensor(),\n        # Normalize the pixel values (in R, G, and B channels)\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    # Load all of the images, transforming them\n    full_dataset = torchvision.datasets.ImageFolder(\n        root=data_path,\n        transform=transformation\n    )\n    \n    \n    # Split into training (70% and testing (30%) datasets)\n    train_size = int(0.7 * len(full_dataset))\n    test_size = len(full_dataset) - train_size\n    \n    # use torch.utils.data.random_split for training/test split\n    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n    \n    # define a loader for the training data we can iterate through in 50-image batches\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=10,\n        num_workers=0,\n        shuffle=False\n    )\n    \n    # define a loader for the testing data we can iterate through in 50-image batches\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=10,\n        num_workers=0,\n        shuffle=False\n    )\n        \n    return train_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:15.767381Z","iopub.execute_input":"2021-10-01T05:11:15.76774Z","iopub.status.idle":"2021-10-01T05:11:15.77718Z","shell.execute_reply.started":"2021-10-01T05:11:15.7677Z","shell.execute_reply":"2021-10-01T05:11:15.776443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recall that we have resized the images and saved them into\ntrain_folder = './images'\n\n# Get the iterative dataloaders for test and training data\ntrain_loader, test_loader = load_dataset(train_folder)\nbatch_size = train_loader.batch_size\nprint(\"Data loaders ready to read\", train_folder)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:15.778702Z","iopub.execute_input":"2021-10-01T05:11:15.779263Z","iopub.status.idle":"2021-10-01T05:11:15.805717Z","shell.execute_reply.started":"2021-10-01T05:11:15.779225Z","shell.execute_reply":"2021-10-01T05:11:15.805019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The folder contains a subfolder for each class of shape\nclasses = sorted(os.listdir(training_folder_name))\nprint(classes)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:15.806975Z","iopub.execute_input":"2021-10-01T05:11:15.807235Z","iopub.status.idle":"2021-10-01T05:11:15.81251Z","shell.execute_reply.started":"2021-10-01T05:11:15.807203Z","shell.execute_reply":"2021-10-01T05:11:15.811444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a neural net class\nclass Net(nn.Module):\n    \n    \n    # Defining the Constructor\n    def __init__(self, num_classes=3):\n        super(Net, self).__init__()\n        \n        # In the init function, we define each layer we will use in our model\n        \n        # Our images are RGB, so we have input channels = 3. \n        # We will apply 12 filters in the first convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n        \n        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n        \n        # We in the end apply max pooling with a kernel size of 2\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        \n        # A drop layer deletes 20% of the features to help prevent overfitting\n        self.drop = nn.Dropout2d(p=0.2)\n        \n        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n        \n        # We need to flatten these in order to feed them to a fully-connected layer\n        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n\n    def forward(self, x):\n        # In the forward function, pass the data through the layers we defined in the init function\n        \n        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n        x = F.relu(self.pool(self.conv1(x))) \n        \n        # Use a ReLU activation function after layer 2\n        x = F.relu(self.pool(self.conv2(x)))  \n        \n        # Select some features to drop to prevent overfitting (only drop during training)\n        x = F.dropout(self.drop(x), training=self.training)\n        \n        # Flatten\n        x = x.view(-1, 32 * 32 * 24)\n        # Feed to fully-connected layer to predict class\n        x = self.fc(x)\n        # Return class probabilities via a log_softmax function \n        return torch.log_softmax(x, dim=1)\n    \ndevice = \"cpu\"\nif (torch.cuda.is_available()):\n    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n    device = \"cuda\"\n\n# Create an instance of the model class and allocate it to the device\nmodel = Net(num_classes=len(classes)).to(device)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:15.814061Z","iopub.execute_input":"2021-10-01T05:11:15.814331Z","iopub.status.idle":"2021-10-01T05:11:21.323984Z","shell.execute_reply.started":"2021-10-01T05:11:15.814299Z","shell.execute_reply":"2021-10-01T05:11:21.323101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, device, train_loader, optimizer, epoch):\n    # Set the model to training mode\n    model.train()\n    train_loss = 0\n    print(\"Epoch:\", epoch)\n    # Process the images in batches\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Use the CPU or GPU as appropriate\n        # Recall that GPU is optimized for the operations we are dealing with\n        data, target = data.to(device), target.to(device)\n        \n        # Reset the optimizer\n        optimizer.zero_grad()\n        \n        # Push the data forward through the model layers\n        output = model(data)\n        \n        # Get the loss\n        loss = loss_criteria(output, target)\n\n        # Keep a running total\n        train_loss += loss.item()\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n        # Print metrics so we see some progress\n        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n            \n    # return average loss for the epoch\n    avg_loss = train_loss / (batch_idx+1)\n    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:21.3254Z","iopub.execute_input":"2021-10-01T05:11:21.325841Z","iopub.status.idle":"2021-10-01T05:11:21.333974Z","shell.execute_reply.started":"2021-10-01T05:11:21.325803Z","shell.execute_reply":"2021-10-01T05:11:21.33319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, device, test_loader):\n    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        batch_count = 0\n        for data, target in test_loader:\n            batch_count += 1\n            data, target = data.to(device), target.to(device)\n            \n            # Get the predicted classes for this batch\n            output = model(data)\n            \n            # Calculate the loss for this batch\n            test_loss += loss_criteria(output, target).item()\n            \n            # Calculate the accuracy for this batch\n            _, predicted = torch.max(output.data, 1)\n            correct += torch.sum(target==predicted).item()\n\n    # Calculate the average loss and total accuracy for this epoch\n    avg_loss = test_loss / batch_count\n    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        avg_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    \n    # return average loss for the epoch\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:21.335357Z","iopub.execute_input":"2021-10-01T05:11:21.335639Z","iopub.status.idle":"2021-10-01T05:11:21.345831Z","shell.execute_reply.started":"2021-10-01T05:11:21.335605Z","shell.execute_reply":"2021-10-01T05:11:21.34493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use an \"Adam\" optimizer to adjust weights\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Specify the loss criteria\nloss_criteria = nn.CrossEntropyLoss()\n\n# Track metrics in these arrays\nepoch_nums = []\ntraining_loss = []\nvalidation_loss = []\n\n# Train over 10 epochs (We restrict to 10 for time issues)\nepochs = 10\nprint('Training on', device)\nfor epoch in range(1, epochs + 1):\n        train_loss = train(model, device, train_loader, optimizer, epoch)\n        test_loss = test(model, device, test_loader)\n        epoch_nums.append(epoch)\n        training_loss.append(train_loss)\n        validation_loss.append(test_loss)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:21.347632Z","iopub.execute_input":"2021-10-01T05:11:21.347966Z","iopub.status.idle":"2021-10-01T05:11:23.236562Z","shell.execute_reply.started":"2021-10-01T05:11:21.347931Z","shell.execute_reply":"2021-10-01T05:11:23.235763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:11:23.237968Z","iopub.execute_input":"2021-10-01T05:11:23.238438Z","iopub.status.idle":"2021-10-01T05:11:23.464832Z","shell.execute_reply.started":"2021-10-01T05:11:23.238387Z","shell.execute_reply":"2021-10-01T05:11:23.464052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}